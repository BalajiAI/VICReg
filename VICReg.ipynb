{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gGVipUtmF2x"
      },
      "source": [
        "# VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCWjCsDYk9iT"
      },
      "source": [
        "In this notebook, we'll be implementing [VICReg: Variance Invariance Covariance Regularization for Self supervised Learning](https://arxiv.org/abs/2105.04906) paper using JAX/Flax framework. VICReg is authored by Adrien Bardes along with  Jean Ponce & Yann LeCun and was published at ICLR 2022.\n",
        "\n",
        "VICReg falls under the category of Non-contrastive methods applied to joint embedding architectures for Self-Supervised Learning (SSL). Despite it's simplicity, it performs on par with other SSL methods and supervised baselines on downstream tasks such as image classification & object detection.\n",
        "\n",
        "**Keywords:** Self-supevised Learning, Representation Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRB_eg68mLg5"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z__2QZrKmQLP"
      },
      "source": [
        "Self Supervised Learning (SSL) is the hottest topic in AI/ML at the moment. SSL is a method for learning representations from unlabeled data. While Self-supervised learning has made a huge success in Natural Language Processing (Eg: BERT, GPT), SSL has shown significant progress only in the recent years (Eg: [DINO](https://arxiv.org/abs/2104.14294)).\n",
        "\n",
        "There are 2 popular methods for SSL applied to Vision, Contrastive Learning and Non-Contrastive Learning methods. Joint embedding architecture (JEA) is a core part behind these 2 methods. In simple terms, JEA consists of two networks which are trained to produce similar embeddings for different views of the same image. Siamese network is a popular instance of JEA, where the two networks share the same weights.\n",
        "\n",
        "In Contrastive learning, JEA is trained to maximize the similarity between different views of the same image (positves) and to minimize the similarity between views of the different images (negatives). The challenge with contrastive learning is it requires a large batch size to work well which is very costly.Whereas in Non-contrastive learning, JEA is just trained to maximize the similarity between positives and no negatives are used. But the main challenge with Non-contrastive learning is to prevent a collapse in which the two networks ignore the inputs and produce identical & constant vectors. Several methods (Eg: [Barlow twins](https://arxiv.org/abs/2103.03230), [W-MSE](https://arxiv.org/abs/2007.06346)) have been proposed to prevent collapse in the context of non-contrastive learning, among which VICReg is insanely simple and yet effective method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYXqqg2emQY4"
      },
      "source": [
        "## VICReg Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63wfiIWYgyRG"
      },
      "source": [
        "![](https://raw.githubusercontent.com/facebookresearch/vicreg/main/.github/vicreg_archi_full.jpg)\n",
        "\n",
        "<center> Illustration of VICReg method <center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHsHW9XoghC"
      },
      "source": [
        "As you can see in the above illustration, two different views (X,X') of the batch of images (I) are generated using transformations (T), which are then passed to the joint embedding architecture to produce the embeddings (Z,Z'). Loss is calculated between the embeddings Z and Z' which needs to be minimized.\n",
        "\n",
        "Next section describes how to compute the loss given the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lULxcUd3uZmX"
      },
      "source": [
        "## VICReg Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEf6rNHdjXDS"
      },
      "source": [
        "The main component of the VICReg method is the Loss which needs to be minimized. The loss is a linear combination of 3 separate losses,\n",
        "\n",
        "\n",
        "1.   **Invariance loss**: mean square distance between Z & Z'.\n",
        "2.   **Variance loss**: a hinge loss to maintain the standard deviation (over a batch) of each variable of the embedding above a given threshold.\n",
        "3.   **Covariance loss**: attracts the covariances (over a batch) between every pair of (centered) embedding variables towards zero.\n",
        "\n",
        "***Use of loss***:\n",
        "Invariance loss helps in increasing the similarity between the embeddings (Z,Z'). Variance loss forces the embeddings of samples within a batch to be different. Covariance loss decorrelates the variables of each embedding or in other words, increases the information content of the embedding.\n",
        "\n",
        "Overall, the loss effectively prevents the collapse.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeCkeB7ibOW3"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CQOo7X1vnur",
        "outputId": "17d0457f-4bee-4b4a-b9dd-b735c037b305"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet flax\n",
        "#Install flaxmodels\n",
        "!pip install --quiet --upgrade git+https://github.com/matthias-wright/flaxmodels.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzqFc8w6kHWp"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import flaxmodels\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb17Mi3Lw9PY"
      },
      "source": [
        "## Config dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV-ZB79yxxpr"
      },
      "outputs": [],
      "source": [
        "#for storing hyperparameters\n",
        "config = {\n",
        "    \"data_path\": \"./Dataset\",\n",
        "    \"hidden_dim\": 1024,\n",
        "    \"ssl_lr\":0.2, # ssl_lr = (batch_size / 256) * base_lr , where base_lr = 0.2\n",
        "    \"lr\": 1e-3,\n",
        "    \"lambda\": 25,\n",
        "    \"mu\": 25,\n",
        "    \"nu\": 1,\n",
        "    \"weight_decay\": 1e-6,\n",
        "    \"num_classes\":10,\n",
        "    \"ssl_batch_size\":256,\n",
        "    \"seed\": 42}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67To5wqMwRLr"
      },
      "source": [
        "## Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kibDDfp3A9YM"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWdcY6ZaVfCp"
      },
      "source": [
        "Torchvision library is used for performing Data augmentation inorder to produce different views of the image. Data augmentation is still performed by CPU, so it can take a significant amount of time to prepare the data. As an alternative, [PIX](https://dm-pix.readthedocs.io/en/latest/) is an image processing library in JAX and can be executed on GPU/TPU to speed up the data preparation. \n",
        "\n",
        "I plan to use PIX instead of torchvision in future versions of this notebook, so stay tuned! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99FxehI0BAyJ"
      },
      "outputs": [],
      "source": [
        "class DataAugmentation(object):\n",
        "  def __init__(self, num_views=2):\n",
        "    self.img_augmentation = transforms.Compose([\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.RandomResizedCrop(size=96),\n",
        "                                        transforms.RandomApply([\n",
        "                                              transforms.ColorJitter(brightness=0.5,\n",
        "                                                                     contrast=0.5,\n",
        "                                                                     saturation=0.5,\n",
        "                                                                     hue=0.1)], p=0.8),\n",
        "                                        transforms.RandomGrayscale(p=0.2),\n",
        "                                        transforms.GaussianBlur(kernel_size=9),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5,), (0.5,))\n",
        "                                              ])\n",
        "    self.num_views = num_views\n",
        "\n",
        "  def __call__(self, img):\n",
        "    return [self.img_augmentation(img) for i in range(self.num_views)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "5963d0fbf27e4b77a2e08d3370474309",
            "99cf3a85b35d4a8286ddaa669dd2d859",
            "cd654d421b4c4b628dd9d419da23138b",
            "ee9b53dbea6c426f81da3fecd372ca87",
            "b26c8a87601542daaae713b58b8a18e9",
            "75563de47bef4b5e83e53d394a41f0e9",
            "d2c48b600b64425bbcb636d3ed3ecbab",
            "8bd560e3d51f4bfc9021068a6f84d486",
            "c1036deef0374bd98b47e28e22b9fc96",
            "270e753f4ca34947828550114bd85a36",
            "e9d4b8e4191f40de94e0a57bf81ff63e"
          ]
        },
        "id": "IAm37B0KwY6i",
        "outputId": "a128338a-c64d-4b76-ffd9-e77b97740abe"
      },
      "outputs": [],
      "source": [
        "unlabeled_data = datasets.STL10(root=config[\"data_path\"], split=\"unlabeled\", download=True, transform=DataAugmentation())\n",
        "unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_data, batch_size = config[\"ssl_batch_size\"], shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEFKmGKCH3iV"
      },
      "source": [
        "### Visualizing augmented images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_3q9FIYI204"
      },
      "outputs": [],
      "source": [
        "num_imgs = 7\n",
        "imgs1 = torch.stack([unlabeled_data[idx][0][0] for idx in range(num_imgs)]) #Only first 7 images in the dataset are visualized\n",
        "grid1 = torchvision.utils.make_grid(imgs1, nrow=7, normalize=True, pad_value=0.6)\n",
        "grid1 = grid1.permute(1, 2, 0)\n",
        "\n",
        "imgs2 = torch.stack([unlabeled_data[idx][0][1] for idx in range(num_imgs)])\n",
        "grid2 = torchvision.utils.make_grid(imgs2, nrow=7, normalize=True, pad_value=0.6)\n",
        "grid2 = grid2.permute(1, 2, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "UaOpBNxSH1RS",
        "outputId": "6feecc61-4782-426c-fc7a-df59125b3192"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,12))\n",
        "plt.title(\"Augmented images - View 1\")\n",
        "plt.imshow(grid1)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(16,12))\n",
        "plt.title(\"Augmented images - View 2\")\n",
        "plt.imshow(grid2)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtVfBnBFwrCT"
      },
      "source": [
        "## VICReg Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pshZyrMvdv1K"
      },
      "source": [
        "### Network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozy5irWIGkyf"
      },
      "source": [
        "Joint embedding architecture consists of two networks. In this notebook, we will be working with Siamese network, so there is no need for another network. The neural network consists of two subnetworks, Encoder and Expander. In our case, encoder is the standard ResNet architecture and Exapander is a MLP where the output dimension is larger than the input dimension.\n",
        "\n",
        "Another important aspect of VICReg is, it does not require that the weights of the two networks be shared, not that the architectures be identical, nor that the inputs be of the same nature. So VICReg can also be extended to the multi-modal setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBcjdu2twaK3"
      },
      "outputs": [],
      "source": [
        "class VICRegNet(nn.Module):\n",
        "  hidden_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.encoder = flaxmodels.ResNet18(output='logits',\n",
        "                                       pretrained=False,\n",
        "                                       normalize=False,\n",
        "                                       num_classes=self.hidden_dim//2)\n",
        "    layers = []\n",
        "    for i in range(2):\n",
        "      layers.append(nn.Dense(self.hidden_dim))\n",
        "      layers.append(nn.BatchNorm(use_running_average=True))\n",
        "      layers.append(nn.relu)\n",
        "    layers.append(nn.Dense(self.hidden_dim))\n",
        "\n",
        "    self.expander = nn.Sequential(layers)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.expander(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UmTuaxNZj0p"
      },
      "source": [
        "### VICReg Self Supervised Tranining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INxxhRvjz5a3"
      },
      "source": [
        "Since BatchNormalization layer is present in ResNet architecture, we need to store batch_statistics along with params (Since JAX can't maintain state). TrainState doesn't provide any option to store the batch_stats, so we create a new class subclassing it inorder to include the batch_stats.\n",
        "\n",
        "I didn't use any learning rate rescheduling, to keep things simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFmnsKk9zozH"
      },
      "outputs": [],
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "  batch_stats: None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0KOHjTY79sA"
      },
      "outputs": [],
      "source": [
        "#Function which returns the off-diagonal elements of a square matrix as a 1-d array\n",
        "#useful in computing covariance loss\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    out = x.flatten()[:-1].reshape(n-1,n+1)[:,1:].flatten()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H1QNlwNsqGZ"
      },
      "outputs": [],
      "source": [
        "class VICRegTrainer:\n",
        "  def __init__(self,\n",
        "               config: dict):\n",
        "    self.model = VICRegNet(hidden_dim=config[\"hidden_dim\"])\n",
        "    self.config = config\n",
        "\n",
        "    self.initialize()\n",
        "    self.create_functions()\n",
        "\n",
        "  def initialize(self):\n",
        "    key = random.PRNGKey(self.config[\"seed\"])\n",
        "    key, sub_key = random.split(key)\n",
        "    sample_input = jnp.ones((1,96,96,3))\n",
        "    variables = self.model.init(key,sample_input)\n",
        "    optimizer = optax.lars(learning_rate=self.config[\"ssl_lr\"], weight_decay=self.config[\"weight_decay\"])\n",
        "    \n",
        "    self.state = TrainState.create(apply_fn=self.model.apply,\n",
        "                                   params = variables[\"params\"],\n",
        "                                   batch_stats = variables[\"batch_stats\"],\n",
        "                                   tx=optimizer)\n",
        "  def create_functions(self):  \n",
        "    def train(dataloader, num_epochs):\n",
        "      for epoch in range(num_epochs):\n",
        "        for (idx, batch) in enumerate(dataloader):\n",
        "          #In PyTorch, image tensors are represented in the shape of (N,C,H,W),\n",
        "          #In Flax, image tensors are represented in the shape of (N,H,W,C),\n",
        "          #so we need to permute the axis accordingly.\n",
        "          x_a = batch[0][0].permute(0,2,3,1).numpy() #Jax don't accept torch tensors as input,\n",
        "          #so we need to convert it into numpy array\n",
        "          x_b = batch[0][1].permute(0,2,3,1).numpy()\n",
        "          grads = jax.grad(vicreg_criterion)(self.state.params, self.state.batch_stats, x_a, x_b)\n",
        "          self.state = self.state.apply_gradients(grads=grads) \n",
        "      \n",
        "        print(f\"Epoch {epoch+1} has completed\")\n",
        "\n",
        "    @jax.jit\n",
        "    def vicreg_criterion(params, batch_stats, x_a, x_b):\n",
        "      #compute representations\n",
        "      z_a,_ = self.model.apply({'params': params, 'batch_stats': batch_stats}, x_a, mutable=[\"batch_stats\"])\n",
        "      z_b,_ = self.model.apply({'params': params, 'batch_stats': batch_stats}, x_b, mutable=[\"batch_stats\"])\n",
        "      \n",
        "      batch_size, embed_dim = z_a.shape[0], z_a.shape[1] #useful for computing covariance loss\n",
        "      \n",
        "      #invariance loss\n",
        "      sim_loss = optax.l2_loss(z_a, z_b).mean() * 2 #matches pytorch's implementation of mse loss\n",
        "\n",
        "      #variance loss\n",
        "      std_z_a = jnp.sqrt(z_a.var(axis=0) + 1e-04)\n",
        "      std_z_b = jnp.sqrt(z_b.var(axis=0) + 1e-04)\n",
        "      std_loss = (jnp.mean(jax.nn.relu(1-std_z_a)) + jnp.mean(jax.nn.relu(1-std_z_b))) * 0.5\n",
        "\n",
        "      #covariance loss\n",
        "      z_a = z_a - z_a.mean(axis=0)\n",
        "      z_b = z_b - z_b.mean(axis=0)\n",
        "      cov_z_a = (z_a.T @ z_a) / (batch_size - 1)\n",
        "      cov_z_b = (z_b.T @ z_b) / (batch_size - 1)\n",
        "      cov_loss = jnp.power(off_diagonal(cov_z_a),2).sum() / embed_dim + jnp.power(off_diagonal(cov_z_b),2).sum() / embed_dim\n",
        "\n",
        "      loss = self.config[\"lambda\"] * sim_loss + self.config[\"mu\"] * std_loss + self.config[\"nu\"] * cov_loss\n",
        "      return loss\n",
        "\n",
        "    self.train = train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX70IX1RdqKn"
      },
      "outputs": [],
      "source": [
        "vicreg_ssl = VICRegTrainer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "WBgfOi8nz0eM",
        "outputId": "ffbd85cc-2348-4ac7-984d-a526b53a3a49"
      },
      "outputs": [],
      "source": [
        "vicreg_ssl.train(unlabeled_dataloader, num_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjb4e8hUAu1U"
      },
      "source": [
        "We've successfully trained the network using VICReg method. However to test whether the network has learned good representations or not, we'll retrain the network on image classification task and compare its performance on test set against the network which is trained from scratch (which is our baseline)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G043O6s7870n"
      },
      "source": [
        "## Comparison to Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9wwWEkBgbd0"
      },
      "source": [
        "### Trainer class for supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmxtS9cu_xsX"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "  def __init__(self,\n",
        "               initialize: bool,\n",
        "               config: dict):\n",
        "    self.model = flaxmodels.ResNet18(output=\"logits\", pretrained=False, normalize=False, num_classes=512)\n",
        "    self.config = config\n",
        "    if initialize:\n",
        "      self.initialize()\n",
        "    self.create_functions()\n",
        "\n",
        "  def initialize(self):\n",
        "    key = random.PRNGKey(self.config[\"seed\"])\n",
        "    key, sub_key = random.split(key)\n",
        "    sample_input = jnp.ones((1,96,96,3))\n",
        "    variables = self.model.init(key,sample_input)\n",
        "    optimizer = optax.adamw(learning_rate=self.config[\"lr\"], weight_decay=self.config[\"weight_decay\"])\n",
        "    \n",
        "    self.state = TrainState.create(apply_fn=self.model.apply,\n",
        "                                   params = variables[\"params\"],\n",
        "                                   batch_stats = variables[\"batch_stats\"],\n",
        "                                   tx=optimizer)\n",
        "  def create_functions(self):  \n",
        "    def train(dataloader, num_epochs):\n",
        "      for epoch in range(num_epochs):\n",
        "        for (idx, batch) in enumerate(dataloader):\n",
        "          imgs, labels = batch\n",
        "          imgs = imgs.permute(0,2,3,1).numpy()\n",
        "          labels = labels.numpy()\n",
        "          grads, (metrics, new_model_state) = jax.grad(classification_criterion, has_aux=True)(self.state.params, self.state.batch_stats, imgs, labels, train=True)\n",
        "          self.state = self.state.apply_gradients(grads=grads, batch_stats=new_model_state['batch_stats']) \n",
        "        print(f\"Epoch {epoch+1} has completed\")\n",
        "\n",
        "    def evaluate(dataloader):\n",
        "      correct = 0 \n",
        "      count = 0 #number of exa in the dataloader \n",
        "      for batch_idx, batch in enumerate(dataloader):\n",
        "        imgs, labels = batch\n",
        "        imgs = imgs.permute(0,2,3,1).numpy()\n",
        "        labels = labels.numpy()\n",
        "        _, (metrics, _) = classification_criterion(self.state.params, self.state.batch_stats, imgs, labels, train=False)\n",
        "        #Increment the count by current batch size\n",
        "        batch_size = batch[0].shape[0] #batch[0] consists of image tensor in which the first index represents batch size\n",
        "        count += batch_size\n",
        "        #Increment the correct by how many labels are predicted right\n",
        "        correct += metrics[\"acc\"] * batch_size\n",
        "   \n",
        "      print(f\"Accuracy on Test set: {correct/count:4.2%} [{int(correct)}/{count}]\")\n",
        "\n",
        "    @jax.tree_util.Partial(jax.jit, static_argnums=4)\n",
        "    def classification_criterion(params, batch_stats, imgs, labels, train: bool):\n",
        "      output = self.model.apply({'params': params, 'batch_stats': batch_stats}, imgs, mutable=[\"batch_stats\"]) #if train else False)\n",
        "      logits, new_model_state = output #if train else (output, None)\n",
        "      loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
        "      acc = (logits.argmax(axis=-1) == labels).mean()\n",
        "      metrics = {'loss': loss.astype(float), 'acc': acc.astype(float)}\n",
        "      return loss, (metrics, new_model_state)\n",
        "    \n",
        "    self.train = train\n",
        "    self.evaluate = evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPLLTLSlbse4"
      },
      "source": [
        "### Training the VICReg model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIw_G92dr3cL",
        "outputId": "97857c2f-efea-4b49-da44-a594affa1e53"
      },
      "outputs": [],
      "source": [
        "#data for supervised learning\n",
        "train_data = datasets.STL10(root=config[\"data_path\"], split=\"train\", download=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))]))\n",
        "test_data = datasets.STL10(root=config[\"data_path\"], split=\"test\", download=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))]))\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=64)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJMJ17daZK0l"
      },
      "outputs": [],
      "source": [
        "vicreg = Trainer(initialize=False, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8ykizCDfkAp"
      },
      "outputs": [],
      "source": [
        "#function for extracting encoder from VICRegNet model\n",
        "\n",
        "def extract_submodule(vicreg):\n",
        "  encoder = vicreg.encoder.clone()\n",
        "  variables = vicreg.encoder.variables\n",
        "  return encoder, variables\n",
        "\n",
        "encoder, pretrained_variables = nn.apply(extract_submodule, vicreg_ssl.model)({'params':vicreg_ssl.state.params})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK2GcMQyiKKi"
      },
      "outputs": [],
      "source": [
        "#model architecture\n",
        "class Classifier(nn.Module):\n",
        "  num_classes: int\n",
        "  backbone: nn.Module\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = self.backbone(x)\n",
        "    x = nn.Dense(self.num_classes, name=\"head\")(x)\n",
        "    return x\n",
        "\n",
        "supervised_model = Classifier(num_classes=10, backbone=encoder)\n",
        "sample_input = jnp.ones((1,96,96,3))\n",
        "variables = supervised_model.init(random.PRNGKey(7),sample_input)\n",
        "optimizer = optax.adamw(learning_rate=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "    \n",
        "#model parameters\n",
        "from flax.core.frozen_dict import freeze\n",
        "#update randomly initialized params with pretrained params from vicreg_ssl\n",
        "variables = variables.unfreeze()\n",
        "variables['params']['backbone'] = pretrained_variables['params']\n",
        "variables = freeze(variables)\n",
        "\n",
        "#Initialize attributes for vicreg\n",
        "vicreg.model = supervised_model.clone()\n",
        "vicreg.state = TrainState.create(apply_fn=vicreg.model.apply, params = variables[\"params\"],batch_stats = variables[\"batch_stats\"],tx=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6I5LWf5x4sw",
        "outputId": "f734035b-f2f6-46a3-b6ee-46697e998e74"
      },
      "outputs": [],
      "source": [
        "vicreg.train(train_loader, num_epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqDgjEaY0i43",
        "outputId": "751b45a9-f420-48ee-c8c2-a5454ce9a588"
      },
      "outputs": [],
      "source": [
        "vicreg.evaluate(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqGK9n6SbRRO"
      },
      "source": [
        "### Training the Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqCgCMmOIXbg"
      },
      "outputs": [],
      "source": [
        "#baseline with initialize set to False\n",
        "\n",
        "baseline = Trainer(initialize=False, config=config)\n",
        "model = Classifier(num_classes=10, backbone=encoder)\n",
        "sample_input = jnp.ones((1,96,96,3))\n",
        "variables = model.init(random.PRNGKey(7),sample_input)\n",
        "optimizer = optax.adamw(learning_rate=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "baseline.model = model\n",
        "baseline.state = TrainState.create(apply_fn=baseline.model.apply, params = variables[\"params\"],batch_stats = variables[\"batch_stats\"],tx=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVxquyDZJsbP",
        "outputId": "e6298606-ff0d-499f-c70c-e5ad7437b291"
      },
      "outputs": [],
      "source": [
        "baseline.train(train_loader, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AshgNXfFI6TT",
        "outputId": "5859a90e-19f2-4388-aee0-8a5c57b84484"
      },
      "outputs": [],
      "source": [
        "baseline.evaluate(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w376hnK9ZqFi"
      },
      "source": [
        "As you can see from the above results, the model trained using VICReg performs better than baseline. It means that the model has learned meanigful representations from unlabelled data via SSL, which leads to its increased performance on labeled data (aka Transfer learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6o7oxoewgSB"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1cY9mWSgFi2"
      },
      "source": [
        "This notebook implemented VICReg method for learning visual representations from unlabeled image data and shows that vicreg model performs better than baseline model on the test set by a reasonable margin.\n",
        "\n",
        "This shows the self supevised learning method's ability to learn meanigful representations from unlabeled data which improves the model's performance on downstream tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3CC6hNqVxXh"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vh8hI1SV2Sn"
      },
      "source": [
        "1.   Adrien Bardes, Jean Ponce, Yann LeCun. VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning. ICLR 2022. ([link](https://arxiv.org/abs/2105.04906))\n",
        "2.   Official codebase for VICReg ([link](https://github.com/facebookresearch/vicreg))\n",
        "3.   Yann LeCun. A Path Towards Autonomous Machine Intelligence. ([link](https://openreview.net/forum?id=BZ5a1r-kVsf&referrer=[the%20profile%20of%20Yann%20LeCun](%2Fprofile%3Fid%3D~Yann_LeCun1)))\n",
        "4.   SimCLR Jax/Flax Tutorial ([link](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial17/SimCLR.html))\n",
        "5.   My blog on Self-supervised Learning ([link](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial17/SimCLR.html))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "270e753f4ca34947828550114bd85a36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5963d0fbf27e4b77a2e08d3370474309": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99cf3a85b35d4a8286ddaa669dd2d859",
              "IPY_MODEL_cd654d421b4c4b628dd9d419da23138b",
              "IPY_MODEL_ee9b53dbea6c426f81da3fecd372ca87"
            ],
            "layout": "IPY_MODEL_b26c8a87601542daaae713b58b8a18e9"
          }
        },
        "75563de47bef4b5e83e53d394a41f0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd560e3d51f4bfc9021068a6f84d486": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99cf3a85b35d4a8286ddaa669dd2d859": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75563de47bef4b5e83e53d394a41f0e9",
            "placeholder": "​",
            "style": "IPY_MODEL_d2c48b600b64425bbcb636d3ed3ecbab",
            "value": "100%"
          }
        },
        "b26c8a87601542daaae713b58b8a18e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1036deef0374bd98b47e28e22b9fc96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd654d421b4c4b628dd9d419da23138b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bd560e3d51f4bfc9021068a6f84d486",
            "max": 2640397119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1036deef0374bd98b47e28e22b9fc96",
            "value": 2640397119
          }
        },
        "d2c48b600b64425bbcb636d3ed3ecbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9d4b8e4191f40de94e0a57bf81ff63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee9b53dbea6c426f81da3fecd372ca87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_270e753f4ca34947828550114bd85a36",
            "placeholder": "​",
            "style": "IPY_MODEL_e9d4b8e4191f40de94e0a57bf81ff63e",
            "value": " 2640397119/2640397119 [01:05&lt;00:00, 39838630.64it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
